{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPa3DzzKXeu2F6xBfVXtbxT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hamza-Chekireb/Artificial-Intelligence-Data-Science-Data-Analysis-Portfolio/blob/main/async_vllm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoW-q60bzojF"
      },
      "outputs": [],
      "source": [
        "! pip install vllm fastapi uvicorn nest-asyncio pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm import LLM, SamplingParams\n",
        "from fastapi import FastAPI\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import asyncio\n",
        "from pydantic import BaseModel\n",
        "\n",
        "#0. Importation of async programming library\n",
        "from uuid import uuid4\n",
        "from vllm import AsyncEngineArgs, AsyncLLMEngine, SamplingParams\n",
        "\n",
        "import time\n",
        "\n",
        "import nest_asyncio"
      ],
      "metadata": {
        "id": "H_lCKrndzujp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app = FastAPI()"
      ],
      "metadata": {
        "id": "Nq6Bo0yxKRWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampling_params = SamplingParams(temperature=0.2, top_k=2, max_tokens=1024, skip_special_tokens = True)"
      ],
      "metadata": {
        "id": "lHXbjWEoB8oO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = LLM(model=\"tiiuae/Falcon3-10B-Instruct\",trust_remote_code=True)  # Publicly available model"
      ],
      "metadata": {
        "id": "CM43h2xK0PEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = llm.generate([\"who are you ?\"], sampling_params = sampling_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MM-ddSCx-iW1",
        "outputId": "0d2c5e01-2541-4127-9f47-011ac4344744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.05s/it, est. speed input: 1.12 toks/s, output: 51.79 toks/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer[0].outputs[0].text"
      ],
      "metadata": {
        "id": "jCU4B9PT_Gl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ngrok config add-authtoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4GpT6Za2tQG",
        "outputId": "3061a03e-89b2-4e4a-8b48-97a2554fdb64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a Pydantic model for the request body\n",
        "\n",
        "\n",
        "# Initialize FastAPI\n",
        "app = FastAPI()\n",
        "\n",
        "@app.post(\"/generate/{request}\")\n",
        "async def generate(request):  # Use the Pydantic model\n",
        "    sampling_params = SamplingParams(temperature=0.2, max_tokens=1024)\n",
        "\n",
        "    # Run the model in a non-blocking manner\n",
        "    # result = await asyncio.to_thread(llm.generate, request, sampling_params)\n",
        "    result = await llm.generate(request, sampling_params)\n",
        "    print(result)\n",
        "    return {\"prompt\": request, \"response\": result[0].outputs[0].text}\n",
        "\n",
        "# Start the server using ngrok\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"Public URL: {public_url}\")\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Start the FastAPI server\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ],
      "metadata": {
        "id": "woFERbmV0dn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize the engine with model and arguments\n",
        "engine_args = AsyncEngineArgs(model=\"tiiuae/Falcon3-10B-Instruct\", enforce_eager=True)\n",
        "model = AsyncLLMEngine.from_engine_args(engine_args)"
      ],
      "metadata": {
        "id": "3ez2JXQWd4sM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "engine = AsyncLLMEngine.from_engine_args(AsyncEngineArgs(model=\"tiiuae/Falcon3-10B-Instruct\",))"
      ],
      "metadata": {
        "id": "YhK3lpJlf9rT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = engine.generate(\"a very long story about armando gorginio politelli ?\", SamplingParams(), request_id=str(uuid4()))"
      ],
      "metadata": {
        "id": "CdcAgvOstskm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown"
      ],
      "metadata": {
        "id": "qPuuT7MRyp_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# outputs = engine.generate(\"tell me a very long story?\", SamplingParams(), request_id=\"2\")\n",
        "engine = AsyncLLMEngine.from_engine_args(AsyncEngineArgs(model=\"tiiuae/Falcon3-10B-Instruct\",))"
      ],
      "metadata": {
        "id": "MXPIIIB8uiX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = SamplingParams(top_k=1, temperature=0.2, max_tokens=1024, stop=[],)"
      ],
      "metadata": {
        "id": "ProDW4-i-AQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "engine = AsyncLLMEngine.from_engine_args(\n",
        "    AsyncEngineArgs(model=\"tiiuae/Falcon3-10B-Instruct\")\n",
        ")\n",
        "\n",
        "# Sampling parameters for text generation\n",
        "params = SamplingParams(top_k=1, temperature=0.2, max_tokens=256, stop=[])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "i4ZkazDDtE2I",
        "outputId": "2561fed2-6344-4a8a-b022-3b2dd218ca78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0502a474-5def-4d17-8049-d4316ce2814a'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "async def main():\n",
        "    engine = AsyncLLMEngine.from_engine_args(AsyncEngineArgs(model=\"tiiuae/Falcon3-10B-Instruct\",))\n",
        "    params = SamplingParams(top_k=1, temperature=0.2, max_tokens=256, stop=[],)\n",
        "\n",
        "    async def run_query(query: str):\n",
        "        # request_id\n",
        "        request_id = str(uuid4())\n",
        "\n",
        "        outputs = engine.generate(query, params, request_id)\n",
        "\n",
        "        async for output in outputs:\n",
        "            final_output = output\n",
        "        responses = [output.text for output in final_output.outputs]\n",
        "        return responses\n",
        "\n",
        "    queries = [\n",
        "        \"What is the capital of France?\",\n",
        "        \"Explain the theory of relativity.\"\n",
        "    ]\n",
        "    tasks = [asyncio.create_task(run_query(q)) for q in queries]\n",
        "    results = await asyncio.gather(*tasks)\n",
        "    for result in results:\n",
        "        print(result)\n",
        "\n",
        "# Use this if you're in a Jupyter notebook or similar environment\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Run the main function\n",
        "await main()\n"
      ],
      "metadata": {
        "id": "XMpabQU4gP2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, Query\n",
        "import asyncio\n",
        "from uuid import uuid4\n",
        "from vllm import AsyncEngineArgs, AsyncLLMEngine, SamplingParams\n",
        "from pyngrok import ngrok\n",
        "import nest_asyncio\n",
        "\n",
        "# Apply nest_asyncio to allow asyncio in environments like Jupyter\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "\n",
        "\n",
        "# Asynchronous query function\n",
        "async def run_query(query: str):\n",
        "    request_id = str(uuid4())\n",
        "    outputs = engine.generate(query, params, request_id)\n",
        "\n",
        "    # Collect the response\n",
        "    async for output in outputs:\n",
        "        final_output = output\n",
        "    responses = [output.text for output in final_output.outputs]\n",
        "    return responses\n",
        "\n",
        "# Define a GET endpoint\n",
        "@app.get(\"/generate/{query}\")\n",
        "async def generate(query: str):\n",
        "    try:\n",
        "        response = await run_query(query)\n",
        "        return {\"query\": query, \"response\": response}\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "# Run the API server using ngrok\n",
        "public_url = ngrok.connect(8000).public_url\n",
        "print(f\"Public API URL: {public_url}\")\n",
        "import uvicorn\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4PVoABEo1cZ",
        "outputId": "0c09748a-dbaf-49b9-f5a6-5d1d25e4f9f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public API URL: https://3f02-34-87-107-239.ngrok-free.app\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [18850]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     2a00:f28:ff4b:13e3:cd16:acab:6891:c5c1:0 - \"GET / HTTP/1.1\" 404 Not Found\n",
            "INFO:     2a00:f28:ff4b:13e3:cd16:acab:6891:c5c1:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
            "INFO:     2a00:f28:ff4b:13e3:cd16:acab:6891:c5c1:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     2a00:f28:ff4b:13e3:cd16:acab:6891:c5c1:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
            "INFO:     135.148.100.147:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     135.148.100.147:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
            "INFO 12-28 15:55:27 async_llm_engine.py:211] Added request 805e7f41-fb50-4906-b53e-77c6422ef85d.\n",
            "INFO 12-28 15:55:27 metrics.py:467] Avg prompt throughput: 0.1 tokens/s, Avg generation throughput: 0.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-28 15:55:28 async_llm_engine.py:211] Added request aa12e4d4-54ec-413d-aa61-0f254afac30c.\n",
            "INFO 12-28 15:55:32 metrics.py:467] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 85.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-28 15:55:37 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.2 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-28 15:55:40 async_llm_engine.py:179] Finished request 805e7f41-fb50-4906-b53e-77c6422ef85d.\n",
            "INFO:     2a00:f28:ff4b:13e3:cd16:acab:6891:c5c1:0 - \"GET /generate/what%20is%20the%20difference%20between%20machine%20learning%20and%20deep%20learning%20in%20detail%20%3F HTTP/1.1\" 200 OK\n",
            "INFO 12-28 15:55:42 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 80.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-28 15:55:47 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 50.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-28 15:55:49 async_llm_engine.py:179] Finished request aa12e4d4-54ec-413d-aa61-0f254afac30c.\n",
            "INFO:     135.148.100.147:0 - \"GET /generate/what%20is%20the%20difference%20between%20machine%20learning%20and%20reinforcement%20learning%20in%20detail%20%3F HTTP/1.1\" 200 OK\n",
            "INFO 12-28 15:55:50 async_llm_engine.py:211] Added request ba13ec48-97e0-41de-9c0b-ab7d0c637aa6.\n",
            "INFO 12-28 15:55:52 metrics.py:467] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 31.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-28 15:55:57 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 50.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-28 15:56:02 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 50.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-28 15:56:07 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 50.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-28 15:56:11 async_llm_engine.py:179] Finished request ba13ec48-97e0-41de-9c0b-ab7d0c637aa6.\n",
            "INFO:     135.148.100.147:0 - \"GET /generate/what%20is%20the%20difference%20between%20machine%20learning%20and%20reinforcement%20learning%20in%20detail%20%3F HTTP/1.1\" 200 OK\n",
            "INFO 12-28 15:56:33 async_llm_engine.py:211] Added request c0786e71-58c3-4015-93a7-f8bf5c4f2375.\n",
            "INFO 12-28 15:56:33 metrics.py:467] Avg prompt throughput: 0.6 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-28 15:56:33 async_llm_engine.py:211] Added request 3e3262c4-403f-42ec-b20a-f1350d26e33e.\n",
            "INFO 12-28 15:56:38 metrics.py:467] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 98.2 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-28 15:56:43 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-28 15:56:48 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-28 15:56:53 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-28 15:56:53 async_llm_engine.py:179] Finished request c0786e71-58c3-4015-93a7-f8bf5c4f2375.\n",
            "INFO:     2a00:f28:ff4b:13e3:cd16:acab:6891:c5c1:0 - \"GET /generate/what%20is%20the%20difference%20between%20machine%20learning%20and%20reinforcement%20learnning%20in%20detail%20%3F HTTP/1.1\" 200 OK\n",
            "INFO 12-28 15:56:54 async_llm_engine.py:179] Finished request 3e3262c4-403f-42ec-b20a-f1350d26e33e.\n",
            "INFO:     135.148.100.147:0 - \"GET /generate/what%20is%20the%20difference%20between%20machine%20learning%20and%20reinforcement%20learning%20in%20detail%20%3F HTTP/1.1\" 200 OK\n",
            "INFO 12-28 16:00:04 async_llm_engine.py:211] Added request f9dba5f3-ac68-4c43-bb1f-6d7d3cda97f7.\n",
            "INFO 12-28 16:00:04 metrics.py:467] Avg prompt throughput: 0.1 tokens/s, Avg generation throughput: 0.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-28 16:00:05 async_llm_engine.py:211] Added request 1f05550b-b054-4446-9ce2-715bd2084fed.\n",
            "INFO 12-28 16:00:09 metrics.py:467] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 87.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-28 16:00:14 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-28 16:00:19 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-28 16:00:24 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-28 16:00:25 async_llm_engine.py:179] Finished request f9dba5f3-ac68-4c43-bb1f-6d7d3cda97f7.\n",
            "INFO:     2a00:f28:ff4b:13e3:cd16:acab:6891:c5c1:0 - \"GET /generate/what%20is%20the%20difference%20between%20machine%20learning%20and%20reinforcement%20learnning%20in%20detail%20%3F HTTP/1.1\" 200 OK\n",
            "INFO 12-28 16:00:26 async_llm_engine.py:179] Finished request 1f05550b-b054-4446-9ce2-715bd2084fed.\n",
            "INFO:     135.148.100.147:0 - \"GET /generate/what%20is%20the%20difference%20between%20machine%20learning%20and%20reinforcement%20learning%20in%20detail%20%3F HTTP/1.1\" 200 OK\n",
            "INFO 12-28 16:01:29 async_llm_engine.py:211] Added request d5536eda-75aa-4489-9297-857cfee45406.\n",
            "INFO 12-28 16:01:29 metrics.py:467] Avg prompt throughput: 0.2 tokens/s, Avg generation throughput: 1.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-28 16:01:33 async_llm_engine.py:211] Added request b43100eb-ce0a-4932-b6c0-3cb34b344e10.\n",
            "INFO 12-28 16:01:34 metrics.py:467] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 62.3 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-28 16:01:39 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-28 16:01:44 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-28 16:01:49 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-28 16:01:50 async_llm_engine.py:179] Finished request d5536eda-75aa-4489-9297-857cfee45406.\n",
            "INFO:     2a00:f28:ff4b:13e3:cd16:acab:6891:c5c1:0 - \"GET /generate/what%20is%20the%20difference%20between%20machine%20learning%20and%20reinforcement%20learnning%20in%20detail%20%3F HTTP/1.1\" 200 OK\n",
            "INFO 12-28 16:01:53 async_llm_engine.py:179] Finished request b43100eb-ce0a-4932-b6c0-3cb34b344e10.\n",
            "INFO:     135.148.100.147:0 - \"GET /generate/what%20is%20the%20difference%20between%20machine%20learning%20and%20reinforcement%20learning%20in%20detail%20%3F HTTP/1.1\" 200 OK\n",
            "INFO 12-28 16:11:27 async_llm_engine.py:211] Added request 19259e53-47fe-4a43-a78a-605ba52334be.\n",
            "INFO 12-28 16:11:27 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-28 16:11:28 async_llm_engine.py:211] Added request 8fc9cd51-f615-46bd-9020-4fbb4e63af25.\n",
            "INFO 12-28 16:11:31 async_llm_engine.py:179] Finished request 8fc9cd51-f615-46bd-9020-4fbb4e63af25.\n",
            "INFO:     135.148.100.147:0 - \"GET /generate/who%20is%20pepe%20gardiola%20in%20detail%20%3F HTTP/1.1\" 200 OK\n",
            "INFO 12-28 16:11:32 metrics.py:467] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 83.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-28 16:11:33 async_llm_engine.py:179] Finished request 19259e53-47fe-4a43-a78a-605ba52334be.\n",
            "INFO:     2a00:f28:ff4b:13e3:cd16:acab:6891:c5c1:0 - \"GET /generate/who%20is%20pepe%20gardiola%20in%20detail%20%3F HTTP/1.1\" 200 OK\n",
            "INFO 12-28 16:12:12 async_llm_engine.py:211] Added request 1a9d8d70-d99b-473b-be3a-7a44925ac0a4.\n",
            "INFO 12-28 16:12:12 metrics.py:467] Avg prompt throughput: 0.1 tokens/s, Avg generation throughput: 1.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-28 16:12:13 async_llm_engine.py:211] Added request f1300671-f0cc-47d6-a760-892f3bc08582.\n",
            "INFO 12-28 16:12:17 metrics.py:467] Avg prompt throughput: 1.0 tokens/s, Avg generation throughput: 89.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-28 16:12:17 async_llm_engine.py:179] Finished request 1a9d8d70-d99b-473b-be3a-7a44925ac0a4.\n",
            "INFO:     2a00:f28:ff4b:13e3:cd16:acab:6891:c5c1:0 - \"GET /generate/who%20is%20jesus%20%3F HTTP/1.1\" 200 OK\n",
            "INFO 12-28 16:12:18 async_llm_engine.py:179] Finished request f1300671-f0cc-47d6-a760-892f3bc08582.\n",
            "INFO:     135.148.100.147:0 - \"GET /generate/who%20is%20jesus%20%3F HTTP/1.1\" 200 OK\n",
            "INFO 12-28 16:25:47 async_llm_engine.py:211] Added request c3e60f9f-2766-4735-8403-c1498405aceb.\n",
            "INFO 12-28 16:25:47 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-28 16:25:48 async_llm_engine.py:211] Added request 1f7ca4d3-c7e4-4aef-824f-0ded23571409.\n",
            "INFO 12-28 16:25:49 async_llm_engine.py:211] Added request 81a08a98-adcb-44f4-8ae8-61c7b95871b9.\n",
            "INFO 12-28 16:25:50 async_llm_engine.py:211] Added request 5a9645b2-627a-40c5-8d6a-e4b111f66122.\n",
            "INFO 12-28 16:25:51 async_llm_engine.py:211] Added request ffc7e6bb-ae5c-4e90-99d4-26777a27a86d.\n",
            "INFO 12-28 16:25:52 metrics.py:467] Avg prompt throughput: 5.2 tokens/s, Avg generation throughput: 150.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-28 16:25:53 async_llm_engine.py:179] Finished request 81a08a98-adcb-44f4-8ae8-61c7b95871b9.\n",
            "INFO 12-28 16:25:55 async_llm_engine.py:179] Finished request 5a9645b2-627a-40c5-8d6a-e4b111f66122.\n",
            "INFO 12-28 16:25:57 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-28 16:26:02 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 146.3 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-28 16:26:07 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 146.6 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.6%, CPU KV cache usage: 0.0%.\n",
            "INFO 12-28 16:26:08 async_llm_engine.py:179] Finished request c3e60f9f-2766-4735-8403-c1498405aceb.\n",
            "INFO 12-28 16:26:09 async_llm_engine.py:179] Finished request 1f7ca4d3-c7e4-4aef-824f-0ded23571409.\n",
            "INFO 12-28 16:26:12 async_llm_engine.py:179] Finished request ffc7e6bb-ae5c-4e90-99d4-26777a27a86d.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from vllm import AsyncLLMEngine, SamplingParams\n",
        "\n",
        "async def main():\n",
        "    # Define the engine arguments\n",
        "    engineargs = EngineArgs(model='facebook/opt-125m')\n",
        "\n",
        "    # Initialize the engine\n",
        "    engine = AsyncLLMEngine.from_engine_args(engineargs)\n",
        "\n",
        "    # Example prompt\n",
        "    example_input = {\n",
        "        \"prompt\": \"What is the capital of France?\",\n",
        "        \"temperature\": 0.7,\n",
        "        \"request_id\": \"req1\",\n",
        "    }\n",
        "\n",
        "    # Generate outputs\n",
        "    results_generator = engine.generate(\n",
        "        prompt=example_input[\"prompt\"],\n",
        "        sampling_params=SamplingParams(temperature=example_input[\"temperature\"]),\n",
        "        request_id=example_input[\"request_id\"]\n",
        "    )\n",
        "\n",
        "    # Process the output\n",
        "    async for output in results_generator:\n",
        "        print(output)\n",
        "\n",
        "# Run the async main function\n",
        "asyncio.run(main())\n"
      ],
      "metadata": {
        "id": "a-_kEJ5UbvX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WjaIofHk7dlc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}