{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hamza-Chekireb/Artificial-Intelligence-Data-Science-Data-Analysis-Portfolio/blob/main/async_vllm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoW-q60bzojF"
      },
      "outputs": [],
      "source": [
        "! pip install vllm fastapi uvicorn nest-asyncio pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_lCKrndzujp"
      },
      "outputs": [],
      "source": [
        "from vllm import LLM, SamplingParams\n",
        "from fastapi import FastAPI\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import asyncio\n",
        "from pydantic import BaseModel\n",
        "\n",
        "#0. Importation of async programming library\n",
        "from uuid import uuid4\n",
        "from vllm import AsyncEngineArgs, AsyncLLMEngine, SamplingParams\n",
        "\n",
        "import time\n",
        "\n",
        "import nest_asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nq6Bo0yxKRWu"
      },
      "outputs": [],
      "source": [
        "app = FastAPI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHXbjWEoB8oO"
      },
      "outputs": [],
      "source": [
        "sampling_params = SamplingParams(temperature=0.2, top_k=2, max_tokens=1024, skip_special_tokens = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CM43h2xK0PEB"
      },
      "outputs": [],
      "source": [
        "llm = LLM(model=\"tiiuae/Falcon3-10B-Instruct\",trust_remote_code=True)  # Publicly available model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MM-ddSCx-iW1",
        "outputId": "0d2c5e01-2541-4127-9f47-011ac4344744"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.05s/it, est. speed input: 1.12 toks/s, output: 51.79 toks/s]\n"
          ]
        }
      ],
      "source": [
        "answer = llm.generate([\"who are you ?\"], sampling_params = sampling_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCU4B9PT_Gl9"
      },
      "outputs": [],
      "source": [
        "answer[0].outputs[0].text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4GpT6Za2tQG",
        "outputId": "3061a03e-89b2-4e4a-8b48-97a2554fdb64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "! ngrok config add-authtoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woFERbmV0dn2"
      },
      "outputs": [],
      "source": [
        "# Define a Pydantic model for the request body\n",
        "\n",
        "\n",
        "# Initialize FastAPI\n",
        "app = FastAPI()\n",
        "\n",
        "@app.post(\"/generate/{request}\")\n",
        "async def generate(request):  # Use the Pydantic model\n",
        "    sampling_params = SamplingParams(temperature=0.2, max_tokens=1024)\n",
        "\n",
        "    # Run the model in a non-blocking manner\n",
        "    # result = await asyncio.to_thread(llm.generate, request, sampling_params)\n",
        "    result = await llm.generate(request, sampling_params)\n",
        "    print(result)\n",
        "    return {\"prompt\": request, \"response\": result[0].outputs[0].text}\n",
        "\n",
        "# Start the server using ngrok\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"Public URL: {public_url}\")\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Start the FastAPI server\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ez2JXQWd4sM"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize the engine with model and arguments\n",
        "engine_args = AsyncEngineArgs(model=\"tiiuae/Falcon3-10B-Instruct\", enforce_eager=True)\n",
        "model = AsyncLLMEngine.from_engine_args(engine_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhK3lpJlf9rT"
      },
      "outputs": [],
      "source": [
        "engine = AsyncLLMEngine.from_engine_args(AsyncEngineArgs(model=\"tiiuae/Falcon3-10B-Instruct\",))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdcAgvOstskm"
      },
      "outputs": [],
      "source": [
        "answer = engine.generate(\"a very long story about armando gorginio politelli ?\", SamplingParams(), request_id=str(uuid4()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPuuT7MRyp_8"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXPIIIB8uiX3"
      },
      "outputs": [],
      "source": [
        "# outputs = engine.generate(\"tell me a very long story?\", SamplingParams(), request_id=\"2\")\n",
        "engine = AsyncLLMEngine.from_engine_args(AsyncEngineArgs(model=\"tiiuae/Falcon3-10B-Instruct\",))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ProDW4-i-AQY"
      },
      "outputs": [],
      "source": [
        "params = SamplingParams(top_k=1, temperature=0.2, max_tokens=1024, stop=[],)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "i4ZkazDDtE2I",
        "outputId": "2561fed2-6344-4a8a-b022-3b2dd218ca78"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0502a474-5def-4d17-8049-d4316ce2814a'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "engine = AsyncLLMEngine.from_engine_args(\n",
        "    AsyncEngineArgs(model=\"tiiuae/Falcon3-10B-Instruct\")\n",
        ")\n",
        "\n",
        "# Sampling parameters for text generation\n",
        "params = SamplingParams(top_k=1, temperature=0.2, max_tokens=256, stop=[])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMpabQU4gP2w"
      },
      "outputs": [],
      "source": [
        "async def main():\n",
        "    engine = AsyncLLMEngine.from_engine_args(AsyncEngineArgs(model=\"tiiuae/Falcon3-10B-Instruct\",))\n",
        "    params = SamplingParams(top_k=1, temperature=0.2, max_tokens=256, stop=[],)\n",
        "\n",
        "    async def run_query(query: str):\n",
        "        # request_id\n",
        "        request_id = str(uuid4())\n",
        "\n",
        "        outputs = engine.generate(query, params, request_id)\n",
        "\n",
        "        async for output in outputs:\n",
        "            final_output = output\n",
        "        responses = [output.text for output in final_output.outputs]\n",
        "        return responses\n",
        "\n",
        "    queries = [\n",
        "        \"What is the capital of France?\",\n",
        "        \"Explain the theory of relativity.\"\n",
        "    ]\n",
        "    tasks = [asyncio.create_task(run_query(q)) for q in queries]\n",
        "    results = await asyncio.gather(*tasks)\n",
        "    for result in results:\n",
        "        print(result)\n",
        "\n",
        "# Use this if you're in a Jupyter notebook or similar environment\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Run the main function\n",
        "await main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4PVoABEo1cZ",
        "outputId": "0c09748a-dbaf-49b9-f5a6-5d1d25e4f9f3"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI, Query\n",
        "import asyncio\n",
        "from uuid import uuid4\n",
        "from vllm import AsyncEngineArgs, AsyncLLMEngine, SamplingParams\n",
        "from pyngrok import ngrok\n",
        "import nest_asyncio\n",
        "\n",
        "# Apply nest_asyncio to allow asyncio in environments like Jupyter\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "\n",
        "\n",
        "# Asynchronous query function\n",
        "async def run_query(query: str):\n",
        "    request_id = str(uuid4())\n",
        "    outputs = engine.generate(query, params, request_id)\n",
        "\n",
        "    # Collect the response\n",
        "    async for output in outputs:\n",
        "        final_output = output\n",
        "    responses = [output.text for output in final_output.outputs]\n",
        "    return responses\n",
        "\n",
        "# Define a GET endpoint\n",
        "@app.get(\"/generate/{query}\")\n",
        "async def generate(query: str):\n",
        "    try:\n",
        "        response = await run_query(query)\n",
        "        return {\"query\": query, \"response\": response}\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "# Run the API server using ngrok\n",
        "public_url = ngrok.connect(8000).public_url\n",
        "print(f\"Public API URL: {public_url}\")\n",
        "import uvicorn\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "# end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-_kEJ5UbvX0"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from vllm import AsyncLLMEngine, SamplingParams\n",
        "\n",
        "async def main():\n",
        "    # Define the engine arguments\n",
        "    engineargs = EngineArgs(model='facebook/opt-125m')\n",
        "\n",
        "    # Initialize the engine\n",
        "    engine = AsyncLLMEngine.from_engine_args(engineargs)\n",
        "\n",
        "    # Example prompt\n",
        "    example_input = {\n",
        "        \"prompt\": \"What is the capital of France?\",\n",
        "        \"temperature\": 0.7,\n",
        "        \"request_id\": \"req1\",\n",
        "    }\n",
        "\n",
        "    # Generate outputs\n",
        "    results_generator = engine.generate(\n",
        "        prompt=example_input[\"prompt\"],\n",
        "        sampling_params=SamplingParams(temperature=example_input[\"temperature\"]),\n",
        "        request_id=example_input[\"request_id\"]\n",
        "    )\n",
        "\n",
        "    # Process the output\n",
        "    async for output in results_generator:\n",
        "        print(output)\n",
        "\n",
        "# Run the async main function\n",
        "asyncio.run(main())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjaIofHk7dlc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPa3DzzKXeu2F6xBfVXtbxT",
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
