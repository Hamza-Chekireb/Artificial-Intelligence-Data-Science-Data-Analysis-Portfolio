{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hamza-Chekireb/Artificial-Intelligence-Data-Science-Data-Analysis-Portfolio/blob/main/Sentiment_analysis_emojifier_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R17X4k_IP93w"
      },
      "source": [
        "**--Atelier 13 octobre 2022 et 21 octobre 2022**\n",
        "\n",
        "**--Text classifier**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "! pip install emoji\n",
        "import emoji\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, SimpleRNN, Embedding"
      ],
      "metadata": {
        "id": "XFDTF6fnr3WB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/Users/Hamza/Downloads/Emojify-Text-main/emoji_data.csv', header = None)"
      ],
      "metadata": {
        "id": "LsucDLQQv3Aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Comment ça marche l'emojize\n",
        "emoji.emojize(':fork_and_knife_with_plate:')\n",
        "\n",
        "# Créer uun dictionnaire pour assigner un emoji à chaque label\n",
        "emoji_dict = {0: \":red_heart:\",    \n",
        "                    1: \":baseball:\",\n",
        "                    2: \":grinning_face_with_big_eyes:\",\n",
        "                    3: \":disappointed_face:\",\n",
        "                    4: \":fork_and_knife_with_plate:\"}\n",
        "\n",
        "# Créer une fonction de conversion de chiffre en emoji\n",
        "\n",
        "def label_to_emoji(label):\n",
        "  return emoji.emojize(emoji_dict[label])\n"
      ],
      "metadata": {
        "id": "0_IRfZfiv6is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualiser les valeurs uniques des Commentaires et les valeurs uniques des étiquettes\n",
        "x = data[0].values\n",
        "y = data[1].values\n",
        "print(x)\n",
        "print(y)\n"
      ],
      "metadata": {
        "id": "LUTmzt46v9WW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#np.unique(y)\n",
        "data[1].unique()\n",
        "\n",
        "data[1].replace({ '0 ': '0', '1 ': '1', '2 ': '2','3 ': '3', '4 ': '4','0v2':'0'}, inplace=True)\n",
        "print(data[1].unique())\n"
      ],
      "metadata": {
        "id": "ZAU59KGvv_kB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('/Users/Hamza/Downloads/glove.6B.50d.txt','r', encoding = 'utf8') # Ouvrir le text qui contient Embedding\n",
        "content = file.readlines()\n",
        "file.close()\n",
        "content"
      ],
      "metadata": {
        "id": "A_FxrQlLwB1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Un Word_Embedding pré_entraîné \n",
        "\n",
        "embeddings = {}\n",
        "for line in content :\n",
        "    # Convertir le Embedding des mots en une liste avec des valeurs de vecteur\n",
        "    line = line.split()# Tokens \n",
        "    # Mettre le premier mot en tant que clé et le reste en tant que valeurs Converties en tableau numpy de valeurs numériques \n",
        "    embeddings[line[0]] = np.array(line[1:], dtype = float)\n",
        "    \n",
        "embeddings    \n",
        "    \n",
        "    "
      ],
      "metadata": {
        "id": "Ss6bamrqwD9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer \n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(x)\n",
        "\n",
        "# Créer le vocabulaire avec les index\n",
        "word2index = tokenizer.word_index\n",
        "word2index"
      ],
      "metadata": {
        "id": "T8S-fv8iwF4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convertir tous les commentaire en séquence d'index\n",
        "xtokens = tokenizer.texts_to_sequences(x)\n",
        "xtokens"
      ],
      "metadata": {
        "id": "LdoIubvgwH_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_maxlen(data):\n",
        "    maxlen = 0\n",
        "    for sent in data:\n",
        "        maxlen = max(maxlen,len(sent))\n",
        "    return maxlen\n",
        "\n",
        "maxlen = get_maxlen(xtokens)"
      ],
      "metadata": {
        "id": "v9-KB50ZwMDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remplir les séquences par des zéros pour qu'ils aient la même longueur \n",
        "from keras.preprocessing.sequence import pad_sequences \n",
        "xtrain = pad_sequences(xtokens, maxlen = maxlen, padding = 'post', truncating='post')\n",
        "xtrain"
      ],
      "metadata": {
        "id": "zEyC3o30wMkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "_spFBK9LwOPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ytrain = to_categorical(y)\n",
        "ytrain"
      ],
      "metadata": {
        "id": "Tuo6XJqfwQJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 50\n",
        "embedding_matrix = np.zeros((len(word2index)+1, embed_size)) # len(word2index)+1 : car le premier indice de word2index est 1\n",
        "embedding_matrix\n",
        "for word,i in word2index.items():\n",
        "    embed_vector = embeddings[word] #embeddings: Embedding_Words préentraîné\n",
        "    embedding_matrix[i] = embed_vector\n",
        "\n",
        "print(embedding_matrix)\n",
        "print(embedding_matrix.shape)"
      ],
      "metadata": {
        "id": "ufg2zChgwT7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    # input_dim : La taille du vocabulaire+1\n",
        "    # output_dim : La taille des vecteurs dans le Word_Embedding utilisé\n",
        "    # input_len : la taille des séquences\n",
        "    # weights : C'est une matrice avec le Word_Embedding de tous les mots du vocabulaire\n",
        "    Embedding(input_dim = len(word2index)+1, output_dim = 50, input_length = maxlen, weights = [embedding_matrix],trainable = False),\n",
        "    SimpleRNN(units=16),\n",
        "    Dense(5, activation= 'softmax')\n",
        "    \n",
        "    \n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "0oKmuDGYwV9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OyMDJR1PwkLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "model.compile(optimizer='adam', loss = tensorflow.keras.metrics.categorical_crossentropy, metrics = ['accuracy'])\n",
        "model.fit(xtrain,ytrain,epochs = 100)"
      ],
      "metadata": {
        "id": "BRmZ7SqjwXx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tester le modèle**"
      ],
      "metadata": {
        "id": "3WUeieJbwaRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test = ['happy men', \"i play baseball\", 'i want to eat']\n",
        "testseq = tokenizer.texts_to_sequences(test)\n",
        "print(testseq)\n",
        "\n",
        "xtest= pad_sequences(testseq, maxlen = maxlen, padding = 'post')\n",
        "xtest"
      ],
      "metadata": {
        "id": "Es2R1AQhweVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ypred = model.predict(xtest)\n",
        "ypred\n",
        "ypred = np.argmax(ypred,axis = 1)\n",
        "ypred"
      ],
      "metadata": {
        "id": "YyL1nHAgwf1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(test)):\n",
        "    print(test[i],label_to_emoji(ypred[i]))"
      ],
      "metadata": {
        "id": "4nH-TE1zwk-a"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1SuW2bcR9PFBiFg6c3RzPZ0DWquMdp-nj",
      "authorship_tag": "ABX9TyPiKCk4fbLEMFLO4zX5eUox",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}